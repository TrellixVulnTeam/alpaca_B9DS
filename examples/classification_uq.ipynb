{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "from alpaca.uncertainty_estimator import build_estimator\n",
    "from alpaca.model.cnn import SimpleConv\n",
    "from alpaca.dataloader.builder import build_dataset\n",
    "from alpaca.analysis.metrics import ndcg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "mnist = build_dataset('mnist', val_size=50_000)\n",
    "x_train, y_train = mnist.dataset('train')\n",
    "x_val, y_val = mnist.dataset('val')\n",
    "x_shape = (-1, 1, 28, 28)\n",
    "\n",
    "train_ds = TensorDataset(torch.FloatTensor(x_train.reshape(x_shape)), torch.Double(y_train))\n",
    "val_ds = TensorDataset(torch.FloatTensor(x_val.reshape(x_shape)), torch.Double(y_val))\n",
    "train_loader = DataLoader(train_ds, batch_size=512)\n",
    "val_loader = DataLoader(val_ds, batch_size=10_000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = SimpleConv().double()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for x_batch, y_batch in train_loader: # Train for one epoch\n",
    "    print('.', end='')\n",
    "    prediction = model(x_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(prediction, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print('\\nTrain loss on last batch', loss.item())\n",
    "\n",
    "# Check accuracy\n",
    "x_batch, y_batch = next(iter(val_loader))\n",
    "\n",
    "class_preds = F.softmax(model(x_batch), dim=-1).detach().numpy()\n",
    "predictions = np.argmax(class_preds, axis=-1)\n",
    "print('Accuracy', accuracy_score(predictions, y_batch))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate uncertainty estimation\n",
    "estimator = build_estimator(\"bald_masked\", model, dropout_mask='mc_dropout', num_classes=10, keep_runs=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.double()\n",
    "estimations = estimator.estimate(x_batch.double())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate NDCG score for the uncertainty\n",
    "errors = [metrics.log_loss(target.reshape(-1, 1), pred.reshape((-1, 10)), labels=list(range(10))) for pred, target in zip(class_preds, y_batch.numpy())]\n",
    "\n",
    "score = ndcg(np.array(errors), estimations)\n",
    "print(\"Quality score is \", score)\n",
    "\n",
    "\n",
    "runs = estimator.last_mcd_runs()\n",
    "sampled_probabilities = softmax(runs, axis=-1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def entropy(x):\n",
    "    return np.sum(-x*np.log(np.clip(x, 1e-8, 1)), axis=-1)\n",
    "\n",
    "\n",
    "def mean_entropy(probabilities):\n",
    "    return entropy(np.mean(probabilities, axis=1))\n",
    "\n",
    "\n",
    "def bald(probabilities):\n",
    "    predictive_entropy = entropy(np.mean(probabilities, axis=1))\n",
    "    expected_entropy = np.mean(entropy(probabilities), axis=1)\n",
    "\n",
    "    return predictive_entropy - expected_entropy\n",
    "\n",
    "def var_ratio(probabilities):\n",
    "    top_classes = np.argmax(probabilities, axis=-1)\n",
    "    # count how many time repeats the strongest class\n",
    "    mode_count = lambda preds : np.max(np.bincount(preds))\n",
    "    modes = [mode_count(point) for point in top_classes]\n",
    "    ue = 1 - np.array(modes) / probabilities.shape[1]\n",
    "    return ue\n",
    "\n",
    "def ensemble_max_prob(probabilities):\n",
    "    mean_probabilities = np.mean(probabilities, axis=1)\n",
    "    top_probabilities = np.max(mean_probabilities, axis=-1)\n",
    "    return 1 - top_probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "probabilities = F.softmax(model(x_batch.double()), dim=-1).detach().numpy()\n",
    "labels = np.array(y_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "methods = {\n",
    "    'bald': bald,\n",
    "    'var_ratio': var_ratio,\n",
    "    'entropy': mean_entropy,\n",
    "    'sampled_max_prob': ensemble_max_prob\n",
    "}\n",
    "\n",
    "def plot_error_detection(probabilities, labels, sampled_probabilities):\n",
    "    \"\"\"\n",
    "    N - number of points in the dataset, C - number of classes, R - number of sampling runs\n",
    "    all arguments expect to be np.array\n",
    "    :param probabilities:  probabilities by model without dropout, NxC\n",
    "    :param labels: true labels for classification, N\n",
    "    :param sampled_probabilities: probabilities sampled by dropout, NxRxC\n",
    "    :return: None, make roc curve plot for error detection\n",
    "    \"\"\"\n",
    "    predictions = np.argmax(probabilities, axis=-1)\n",
    "    errors = (labels != predictions).astype('uint8')\n",
    "\n",
    "    for name, method_function  in methods.items():\n",
    "        fpr, tpr, _ = roc_curve(errors, method_function(sampled_probabilities))\n",
    "        plt.plot(fpr, tpr, label=name)\n",
    "    max_prob = 1 - np.max(probabilities, axis=-1)\n",
    "    fpr, tpr, _ = roc_curve(errors, max_prob)\n",
    "    plt.plot(fpr, tpr, label='max_prob')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plot_error_detection(probabilities, labels, sampled_probabilities, methods)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}