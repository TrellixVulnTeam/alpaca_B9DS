
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>alpaca.ue.masks &#8212; alpaca 0.1 documentation</title>
    <link rel="stylesheet" href="../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for alpaca.ue.masks</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">abc</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="k">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">alpaca.utils.functions</span> <span class="k">import</span> <span class="n">corrcoef</span><span class="p">,</span> <span class="n">mc_probability</span><span class="p">,</span> <span class="n">cov</span>

<span class="n">reg_masks</span> <span class="o">=</span> <span class="p">{}</span>


<span class="k">class</span> <span class="nc">BaseMask</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">abc</span><span class="o">.</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_name_collection</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">reg_masks</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The given mask name: </span><span class="se">\</span>
<span class="s2">                            `</span><span class="si">{}</span><span class="s2">` exists&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">name</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="n">reg_masks</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="bp">cls</span>
        <span class="n">instance</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">BaseMask</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">instance</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">layer_num</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        TODO: docs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="o">...</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">dry_run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">X_pool</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        TODO: docs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="o">...</span>


<span class="k">class</span> <span class="nc">MaskLayered</span><span class="p">(</span><span class="n">BaseMask</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_correlations</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        TODO: docs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="o">...</span>


<div class="viewcode-block" id="BasicBernoulliMask"><a class="viewcode-back" href="../../../masks.html#alpaca.ue.masks.BasicBernoulliMask">[docs]</a><span class="k">class</span> <span class="nc">BasicBernoulliMask</span><span class="p">(</span><span class="n">BaseMask</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TODO:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_name_collection</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mc_dropout&quot;</span><span class="p">,</span> <span class="s2">&quot;basic_bern&quot;</span><span class="p">}</span>
    <span class="c1"># TODO: this creates some ambiguities</span>
    <span class="c1"># maybe keeping the single value would be optimal</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">layer_num</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="nb">int</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># TODO: remove this, we need to think of better OOP arch here</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">dropout_rate</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="mf">1.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Dropout probability has to be between 0 and 1, &quot;</span>
                <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">item</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
            <span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
            <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span>

<div class="viewcode-block" id="BasicBernoulliMask.dry_run"><a class="viewcode-back" href="../../../masks.html#alpaca.ue.masks.BasicBernoulliMask.dry_run">[docs]</a>    <span class="k">def</span> <span class="nf">dry_run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">X_pool</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">pass</span></div></div>


<div class="viewcode-block" id="DecorrelationMask"><a class="viewcode-back" href="../../../masks.html#alpaca.ue.masks.DecorrelationMask">[docs]</a><span class="k">class</span> <span class="nc">DecorrelationMask</span><span class="p">(</span><span class="n">MaskLayered</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TODO:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_name_collection</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;decorrelating&quot;</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">scaling</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">ht_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="n">scaling</span>  <span class="c1"># use adaptive scaling before softmax</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ht_norm</span> <span class="o">=</span> <span class="n">ht_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">layer_num</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">mask_len</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">mask_len</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">layer_num</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_correlations</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_layers</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">layer_num</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">mask_len</span><span class="p">)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mask_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">inds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer_correlations</span><span class="p">[</span><span class="n">layer_num</span><span class="p">],</span> <span class="n">k</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ht_norm</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">mask</span><span class="p">[</span><span class="n">inds</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="n">layer_num</span><span class="p">][</span><span class="n">inds</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mask</span><span class="p">[</span><span class="n">inds</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mask</span>

    <span class="k">def</span> <span class="nf">_init_layers</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">layer_num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mask_len</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span>
        <span class="n">corrs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">corrcoef</span><span class="p">((</span><span class="n">x</span> <span class="o">+</span> <span class="n">noise</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">())),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">corrs</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="p">(</span>
                <span class="mf">4.0</span> <span class="o">*</span> <span class="n">scores</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
            <span class="p">)</span>  <span class="c1"># TODO: remove hard coding or annotate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_correlations</span><span class="p">[</span><span class="n">layer_num</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ht_norm</span><span class="p">:</span>
            <span class="c1"># Horvitz-Thopson normalization (1 / marginal_prob for each element)</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_correlations</span><span class="p">[</span><span class="n">layer_num</span><span class="p">]</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># TODO: why? (explain 1000)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="n">layer_num</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span>
                <span class="n">mc_probability</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Initially we should pass identity mask,</span>
        <span class="c1"># otherwise we won&#39;t get right correlations for all layers</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">mask_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<div class="viewcode-block" id="DecorrelationMask.reset"><a class="viewcode-back" href="../../../masks.html#alpaca.ue.masks.DecorrelationMask.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_correlations</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span></div>

<div class="viewcode-block" id="DecorrelationMask.dry_run"><a class="viewcode-back" href="../../../masks.html#alpaca.ue.masks.DecorrelationMask.dry_run">[docs]</a>    <span class="k">def</span> <span class="nf">dry_run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">X_pool</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="n">net</span><span class="p">(</span><span class="n">X_pool</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">dropout_mask</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="DecorrelationMaskScaled"><a class="viewcode-back" href="../../../masks.html#alpaca.ue.masks.DecorrelationMaskScaled">[docs]</a><span class="k">class</span> <span class="nc">DecorrelationMaskScaled</span><span class="p">(</span><span class="n">MaskLayered</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TODO:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_name_collection</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;decorrelating_sc&quot;</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="DecorrelationMaskHT"><a class="viewcode-back" href="../../../masks.html#alpaca.ue.masks.DecorrelationMaskHT">[docs]</a><span class="k">class</span> <span class="nc">DecorrelationMaskHT</span><span class="p">(</span><span class="n">MaskLayered</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TODO:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_name_collection</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;ht_decorrelating&quot;</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ht_norm</span> <span class="o">=</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="LeverageScoreMask"><a class="viewcode-back" href="../../../masks.html#alpaca.ue.masks.LeverageScoreMask">[docs]</a><span class="k">class</span> <span class="nc">LeverageScoreMask</span><span class="p">(</span><span class="n">MaskLayered</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TODO:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">ht_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">lambda_</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">covariance</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ht_norm</span> <span class="o">=</span> <span class="n">ht_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">=</span> <span class="n">lambda_</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">covariance</span> <span class="o">=</span> <span class="n">covariance</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">layer_num</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">mask_len</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">mask_len</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">layer_num</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_correlations</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_layers</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mask_len</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="p">),</span> <span class="n">k</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_correlations</span><span class="p">[</span><span class="n">layer_num</span><span class="p">],</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ht_norm</span><span class="p">:</span>
            <span class="n">mask</span><span class="p">[</span><span class="n">ids</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="n">layer_num</span><span class="p">][</span><span class="n">ids</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mask</span><span class="p">[</span><span class="n">ids</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mask</span>

    <span class="k">def</span> <span class="nf">_init_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">layer_num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mask_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">covariance</span><span class="p">:</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">cov</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">corrcoef</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">leverages_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">K</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">*</span> <span class="n">identity</span><span class="p">))</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">leverages_matrix</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">probabilities</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_correlations</span><span class="p">[</span><span class="n">layer_num</span><span class="p">]</span> <span class="o">=</span> <span class="n">probabilities</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ht_norm</span><span class="p">:</span>
            <span class="c1"># Horvitz-Thopson normalization (1 / marginal_prob for each element)</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_correlations</span><span class="p">[</span><span class="n">layer_num</span><span class="p">]</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># TODO: why? (explain 1000)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">[</span><span class="n">layer_num</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span>
                <span class="n">mc_probability</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Initially we should pass identity mask,</span>
        <span class="c1"># otherwise we won&#39;t get right correlations for all layers</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">mask_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<div class="viewcode-block" id="LeverageScoreMask.reset"><a class="viewcode-back" href="../../../masks.html#alpaca.ue.masks.LeverageScoreMask.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_correlations</span> <span class="o">=</span> <span class="p">{}</span></div>

<div class="viewcode-block" id="LeverageScoreMask.dry_run"><a class="viewcode-back" href="../../../masks.html#alpaca.ue.masks.LeverageScoreMask.dry_run">[docs]</a>    <span class="k">def</span> <span class="nf">dry_run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">X_pool</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="n">net</span><span class="p">(</span><span class="n">X_pool</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">dropout_mask</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span></div></div>


<span class="k">class</span> <span class="nc">LeverageScoreMaskHT</span><span class="p">(</span><span class="n">LeverageScoreMask</span><span class="p">):</span>
    <span class="n">_name_collection</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;ht_leverages&quot;</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ht_norm</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span>


<span class="k">class</span> <span class="nc">LeverageScoreMaskCov</span><span class="p">(</span><span class="n">LeverageScoreMask</span><span class="p">):</span>
    <span class="n">_name_collection</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;cov_leverages&quot;</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ht_norm</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">covariance</span> <span class="o">=</span> <span class="kc">True</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">class DPPMask:</span>
<span class="sd">    def __init__(self, ht_norm=False, covariance=False):</span>
<span class="sd">        self.dpps = {}</span>
<span class="sd">        self.layer_correlations = (</span>
<span class="sd">            {}</span>
<span class="sd">        )  # keep for debug purposes # Flag for uncertainty estimator to make first run without taking the result</span>
<span class="sd">        self.dry_run = True</span>
<span class="sd">        self.ht_norm = ht_norm</span>
<span class="sd">        self.norm = {}</span>
<span class="sd">        self.covariance = covariance</span>

<span class="sd">    def __call__(self, x, dropout_rate=0.5, layer_num=0):</span>
<span class="sd">        if layer_num not in self.layer_correlations:</span>
<span class="sd">            # warm-up, generatign correlations masks</span>
<span class="sd">            x_matrix = x.cpu().numpy()</span>

<span class="sd">            self.x_matrix = x_matrix</span>
<span class="sd">            micro = 1e-12</span>
<span class="sd">            x_matrix += (</span>
<span class="sd">                np.random.random(x_matrix.shape) * micro</span>
<span class="sd">            )  # for computational stability</span>
<span class="sd">            if self.covariance:</span>
<span class="sd">                L = np.cov(x_matrix.T)</span>
<span class="sd">            else:</span>
<span class="sd">                L = np.corrcoef(x_matrix.T)</span>

<span class="sd">            self.dpps[layer_num] = FiniteDPP(&quot;likelihood&quot;, **{&quot;L&quot;: L})</span>
<span class="sd">            self.layer_correlations[layer_num] = L</span>

<span class="sd">            if self.ht_norm:</span>
<span class="sd">                L = torch.DoubleTensor(L).cuda()</span>
<span class="sd">                I = torch.eye(len(L)).double().cuda()</span>
<span class="sd">                K = torch.mm(L, torch.inverse(L + I))</span>

<span class="sd">                self.norm[layer_num] = torch.reciprocal(</span>
<span class="sd">                    torch.diag(K)</span>
<span class="sd">                )  # / len(correlations)</span>
<span class="sd">                self.L = L</span>
<span class="sd">                self.K = K</span>

<span class="sd">            return x.data.new(x.data.size()[-1]).fill_(1)</span>

<span class="sd">        # sampling nodes ids</span>
<span class="sd">        dpp = self.dpps[layer_num]</span>

<span class="sd">        for _ in range(ATTEMPTS):</span>
<span class="sd">            dpp.sample_exact()</span>
<span class="sd">            ids = dpp.list_of_samples[-1]</span>
<span class="sd">            if len(ids):  # We should retry if mask is zero-length</span>
<span class="sd">                break</span>

<span class="sd">        mask_len = x.shape[-1]</span>
<span class="sd">        mask = torch.zeros(mask_len).double().cuda()</span>
<span class="sd">        if self.ht_norm:</span>
<span class="sd">            mask[ids] = self.norm[layer_num][ids]</span>
<span class="sd">        else:</span>
<span class="sd">            mask[ids] = mask_len / len(ids)</span>

<span class="sd">        return x.data.new(mask)</span>

<span class="sd">    def reset(self):</span>
<span class="sd">        self.layer_correlations = {}</span>


<span class="sd">def get_nu(eigen_values, k):</span>
<span class="sd">    values = eigen_values + 1e-14</span>

<span class="sd">    def point(nu):</span>
<span class="sd">        exp_nu = np.exp(nu)</span>
<span class="sd">        expect = np.sum([val * exp_nu / (1 + exp_nu * val) for val in values])</span>
<span class="sd">        return expect - k</span>

<span class="sd">    try:</span>
<span class="sd">        solution = root_scalar(point, bracket=[-10.0, 10.0])</span>
<span class="sd">        assert solution.converged</span>
<span class="sd">        return solution.root</span>
<span class="sd">    except (ValueError, AssertionError):</span>
<span class="sd">        raise ValueError(&quot;k-dpp: Probably too small matrix rank for the k&quot;)</span>


<span class="sd">class KDPPMask:</span>
<span class="sd">    def __init__(</span>
<span class="sd">        self, noise_level=None, tol_level=1e-3, ht_norm=False, covariance=False</span>
<span class="sd">    ):</span>
<span class="sd">        self.layer_correlations = {}</span>
<span class="sd">        self.dry_run = True</span>
<span class="sd">        self.dpps = {}</span>
<span class="sd">        self.ranks = {}</span>
<span class="sd">        self.ranks_history = defaultdict(list)</span>
<span class="sd">        self.noise_level = noise_level</span>
<span class="sd">        self.tol_level = tol_level</span>

<span class="sd">        self.ht_norm = ht_norm</span>
<span class="sd">        self.norm = {}</span>

<span class="sd">        self.covariance = covariance</span>

<span class="sd">    def _rank(self, dpp=None, eigen_values=None):</span>
<span class="sd">        if eigen_values is None:</span>
<span class="sd">            eigen_values = dpp.L_eig_vals</span>
<span class="sd">        rank = np.count_nonzero(eigen_values &gt; self.tol_level)</span>
<span class="sd">        return rank</span>

<span class="sd">    def __call__(self, x, dropout_rate=0.5, layer_num=0):</span>
<span class="sd">        mask_len = x.shape[-1]</span>
<span class="sd">        k = int(mask_len * (1 - dropout_rate))</span>

<span class="sd">        if layer_num not in self.layer_correlations:</span>
<span class="sd">            x_matrix = x.cpu().numpy()</span>

<span class="sd">            if self.covariance:</span>
<span class="sd">                L = np.cov(x_matrix.T)</span>
<span class="sd">            else:</span>
<span class="sd">                L = np.corrcoef(x_matrix.T)</span>

<span class="sd">            if self.noise_level is not None:</span>
<span class="sd">                L += self.noise_level * np.eye(len(L))</span>

<span class="sd">            if not self.ht_norm:</span>
<span class="sd">                self.dpps[layer_num] = FiniteDPP(&quot;likelihood&quot;, **{&quot;L&quot;: L})</span>
<span class="sd">                self.dpps[layer_num].sample_exact()  # to trigger eig values generation</span>
<span class="sd">            else:</span>
<span class="sd">                eigen_values = np.linalg.eigh(L)[0]</span>
<span class="sd">                &quot;Get tilted k-dpp, see amblard2018&quot;</span>
<span class="sd">                nu = get_nu(eigen_values, k)</span>
<span class="sd">                I = torch.eye(len(L)).to(x.device)</span>
<span class="sd">                L_tilted = np.exp(nu) * torch.DoubleTensor(L).to(x.device)</span>
<span class="sd">                K_tilted = torch.mm(L_tilted, torch.inverse(L_tilted + I)).double()</span>
<span class="sd">                self.dpps[layer_num] = FiniteDPP(</span>
<span class="sd">                    &quot;correlation&quot;, **{&quot;K&quot;: K_tilted.detach().cpu().numpy()}</span>
<span class="sd">                )</span>
<span class="sd">                self.norm[layer_num] = torch.reciprocal(torch.diag(K_tilted))</span>
<span class="sd">                self.L = L_tilted</span>
<span class="sd">                self.K = K_tilted</span>

<span class="sd">            # Keep data for debugging</span>
<span class="sd">            self.layer_correlations[layer_num] = L</span>
<span class="sd">            mask = torch.ones(mask_len).double().to(x.device)</span>

<span class="sd">            return mask</span>

<span class="sd">        mask = torch.zeros(mask_len).double().to(x.device)</span>
<span class="sd">        ids = self.dpps[layer_num].sample_exact_k_dpp(k)</span>

<span class="sd">        if self.ht_norm:</span>
<span class="sd">            mask[ids] = self.norm[layer_num][ids]</span>
<span class="sd">        else:</span>
<span class="sd">            mask[ids] = mask_len / len(ids)</span>

<span class="sd">        return mask</span>

<span class="sd">    def reset(self):</span>
<span class="sd">        self.layer_correlations = {}</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">alpaca</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../ue.html">Uncertainty estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../masks.html">Masks</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.2.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
    </div>

    

    
  </body>
</html>